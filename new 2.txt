You want to use your Ubuntu machine as a "command center" to manage and run the cloud Actor you already built on the Apify platform.

This is the standard and most powerful way to work. You'll use the Apify CLI (Command Line Interface) to link a local project directory on your Ubuntu machine directly to the cloud Actor. This lets you edit the code locally in your preferred editor, push changes with one command, and, most importantly, launch runs from your terminal.

Here is the turnkey solution for that environment.

1. ‚öôÔ∏è Install the Apify CLI on Ubuntu
The Apify CLI is a Node.js application. You'll need to install npm (Node Package Manager) first.

Bash

# 1. Update your package lists
sudo apt update

# 2. Install Node.js and npm
# This command installs both
sudo apt install nodejs npm

# 3. Verify the installation
node -v
npm -v

# 4. Install the Apify CLI globally using npm
sudo npm install apify-cli -g

# 5. Verify the CLI installation
apify --version
2. üîë Log In and Link Your Account
Now, you need to connect the CLI to your Apify account.

Bash

# 1. Run the login command
apify login
This will ask for your Apify API Token.

Go to your Apify account settings: https://console.apify.com/account/integrations

Find your API Token (it looks like apify_api_...).

Copy and paste it into the terminal.

3. üìÇ Pull Your Existing Cloud Actor
This is the most important step. We will "pull" the adcrr-http-scraper Actor you already built, which will create a local copy and automatically link it.

Bash

# 1. Make a new directory on your machine to hold the project
mkdir ~/projects/adcrr-apify-actor
cd ~/projects/adcrr-apify-actor

# 2. Pull the Actor from the cloud
# Replace 'your-actor-name' with the exact name you gave it
# (e.g., adcrr-http-scraper)
apify pull adcrr-http-scraper
Your directory will now contain all the files you created in the web UI:

main.py

requirements.txt

INPUT_SCHEMA.json

.actor/apify.json (This is the crucial file that links this folder to the cloud)

4. üöÄ Run the Actor from Your Terminal
You can now run your cloud Actor from this directory. The command apify run does not run the code locally. It tells the Apify cloud to execute a new run, which is exactly what you want.

To run a single test batch:

The -i flag passes your JSON input. Note the single quotes around the JSON.

Bash

# Make sure you are in your project directory
cd ~/projects/adcrr-apify-actor

# Run a test batch from 1 to 10
apify run -i '{"start_adc_number": 1, "end_adc_number": 10}'
This will start the run in the cloud and stream the logs directly to your terminal. Press Ctrl+C to stop streaming the logs (the run will continue in the cloud).

5. ü§ñ Scripting Your Full 375,000-Record Scrape
This is the real power. You can now write a simple bash script to launch all 38 parallel runs at once.

Create a file named launch_all.sh:

Bash

nano launch_all.sh
Paste this script into the file. This loops and launches all your batches (from 1 to 380,000, in batches of 10,000).

Bash

#!/bin/bash

echo "--- Launching all ADCRR scraper batches ---"

BATCH_SIZE=10000
TOTAL_RECORDS=380000 # Run a bit over 375k just in case

# Loop from 1 up to TOTAL_RECORDS, in steps of BATCH_SIZE
for ((START=1; START<=TOTAL_RECORDS; START+=BATCH_SIZE))
do
  # Calculate the end number for this batch
  END=$((START + BATCH_SIZE - 1))

  echo "Launching batch: $START to $END"

  # Build the input JSON string
  INPUT_JSON="{\"start_adc_number\": $START, \"end_adc_number\": $END}"

  # Launch the Apify run. 
  # The '-b' flag runs it in the background (doesn't stream logs)
  # so the script can immediately launch the next one.
  apify run -b -i "$INPUT_JSON"

  # Sleep for a second to avoid spamming the Apify API
  sleep 1
done

echo "--- All batches launched! ---"
echo "Check the Apify Console to monitor the runs."
Save and exit (Ctrl+X, Y, Enter).

Make the script executable:

Bash

chmod +x launch_all.sh
Run the script to launch everything:

Bash

./launch_all.sh
That's it. Your Ubuntu terminal will loop for about 40 seconds, launching all 38 runs in the Apify cloud. You can then close your terminal and go to the Apify web console to watch them all run in parallel. All the data will be collected in that Actor's single default dataset